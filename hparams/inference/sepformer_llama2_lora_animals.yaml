seed: 1234
__set_seed: !apply:torch.manual_seed [!ref <seed>]
np_rng: !new:numpy.random.RandomState [!ref <seed>]

save_folder: !PLACEHOLDER
analyze: false

# Data

data_root: !PLACEHOLDER
filt_labels: !apply:data.datasets.extraction.get_animal_labels_in_vggsound
filt_labels_mode: both
keep_spks: true

test_set: !new:data.datasets.extraction.AudioMixtures
    data_root: !ref <data_root>
    manifest_files: ['data/manifests/animals_test.json']
    keep_spks: !ref <keep_spks>
    filt_labels: !ref <filt_labels>
    filt_labels_mode: !ref <filt_labels_mode>

# Loader

test_loader_opts:
    batch_size: 1
    shuffle: false

# Speedup

mix_prec: false
mix_dtype: !name:torch.bfloat16 # always
llm_mix_prec: false

# Model

enc_chan: 256
kernel_size: 16
kernel_stride: !ref <kernel_size> // 2
chunk_size: 250
n_block: 2
d_ffn: 1024
n_head: 8
n_transformer_layer: 8

Encoder: !new:speechbrain.lobes.models.dual_path.Encoder
    kernel_size: !ref <kernel_size>
    out_channels: !ref <enc_chan>

SBtfintra: !new:speechbrain.lobes.models.dual_path.SBTransformerBlock
  num_layers: !ref <n_transformer_layer>
  d_model: !ref <enc_chan>
  nhead: !ref <n_head>
  d_ffn: !ref <d_ffn>
  dropout: 0
  use_positional_encoding: true
  norm_before: true

SBtfinter: !new:speechbrain.lobes.models.dual_path.SBTransformerBlock
  num_layers: !ref <n_transformer_layer>
  d_model: !ref <enc_chan>
  nhead: !ref <n_head>
  d_ffn: !ref <d_ffn>
  dropout: 0
  use_positional_encoding: true
  norm_before: true

# FiLM

film_mode: 'block'
film_n_layer: 2
film_scale: true
film_where: 'before1x1'

# new!!!
use_mask: true

MaskNet: !new:modules.dual_path_ext.Dual_Path_Model
    num_spks: 1 # fixed
    in_channels: !ref <enc_chan>
    out_channels: !ref <enc_chan>
    num_layers: !ref <n_block>
    K: !ref <chunk_size>
    intra_model: !ref <SBtfintra>
    inter_model: !ref <SBtfinter>
    norm: ln
    linear_layer_after_inter_intra: false
    skip_around_intra: true
    cond_dim: !ref <txt_emb_dim>
    film_mode: !ref <film_mode>
    film_n_layer: !ref <film_n_layer>
    film_scale: !ref <film_scale>
    analyze: !ref <analyze>

Decoder: !new:speechbrain.lobes.models.dual_path.Decoder
    in_channels: !ref <enc_chan>
    out_channels: 1
    kernel_size: !ref <kernel_size>
    stride: !ref <kernel_stride>
    bias: false

txt_emb_dim: 4096

# Text encoder

llm_path: /engram/naplab/shared/LLaMA2/huggingface/Llama-2-7b-chat-hf

tokenizer: !apply:transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: !ref <llm_path>
    add_eos_token: !ref <add_eos>

llm: !apply:transformers.AutoModelForCausalLM.from_pretrained
    pretrained_model_name_or_path: !ref <llm_path>

add_eos: true 

# LoRA

lora_modules: ['q_proj', 'v_proj']
lora_r: 16
lora_alpha: !ref <lora_r>
lora_dropout: 0.05

lora_config: !new:peft.LoraConfig
    r: !ref <lora_r>
    lora_alpha: !ref <lora_alpha>
    target_modules: !ref <lora_modules>
    lora_dropout: !ref <lora_dropout>
    bias: "none"

lora_llm: !apply:peft.get_peft_model
    model: !ref <llm>
    peft_config: !ref <lora_config>

# Everything

modules:
    encoder: !ref <Encoder>
    decoder: !ref <Decoder>
    masknet: !ref <MaskNet>
    lora_llm: !ref <lora_llm>

# Log and save

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    custom_load_hooks:
        lora_llm: !name:utils.lora_ckpt.load_lora
    custom_save_hooks:
        lora_llm: !name:utils.lora_ckpt.save_lora
    recoverables:
        encoder: !ref <Encoder>
        decoder: !ref <Decoder>
        masknet: !ref <MaskNet>
        lora_llm: !ref <lora_llm>